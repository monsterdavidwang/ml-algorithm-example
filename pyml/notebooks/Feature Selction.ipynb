{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Constant Features from Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constant_features = [feat for feat in X_train.columns if X_train[feat].std() == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.drop(labels=constant_features, axis=1, inplace=True)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect and Remove Duplicate Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Check for duplicated features in the training set\n",
    "duplicated_feat = []\n",
    "for i in range(0, len(X_train.columns)):\n",
    "    if i % 10 == 0:  # this helps me understand how the loop is going\n",
    "        print(i)\n",
    " \n",
    "    col_1 = X_train.columns[i]\n",
    " \n",
    "    for col_2 in X_train.columns[i + 1:]:\n",
    "        if X_train[col_1].equals(X_train[col_2]):\n",
    "            duplicated_feat.append(col_2)\n",
    "            \n",
    "len(duplicated_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicated features\n",
    "X_train.drop(labels=duplicated_feat, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find and remove correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation(dataset, threshold):\n",
    "    col_corr = set()  # Set of all the names of correlated columns\n",
    "    corr_matrix = dataset.corr()\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i):\n",
    "            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value\n",
    "                colname = corr_matrix.columns[i]  # getting the name of column\n",
    "                col_corr.add(colname)\n",
    "    return col_corr\n",
    " \n",
    "corr_features = correlation(X_train, 0.8)\n",
    "print('correlated features: ', len(set(corr_features)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.drop(labels=corr_features, axis=1, inplace=True)\n",
    "X_test.drop(labels=corr_features, axis=1, inplace=True)\n",
    " \n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threshold based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## variance threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "data = ...\n",
    "var_thresh = VarianceThreshold(threshold=0.1)\n",
    "transformed_data = var_thresh.fit_transform(data)\n",
    "# transformed data will have all columns with variance less\n",
    "# than 0.1 removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Method\n",
    "1. Mutual Information\n",
    "2. Anova\n",
    "3. chi2-square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "\n",
    "\n",
    "class UnivariateFeatureSelction:\n",
    "    def __init__(self, n_features, problem_type, scoring):\n",
    "        \"\"\"\n",
    "        Custom univariate feature selection wrapper on\n",
    "        different univariate feature selection models from\n",
    "        scikit-learn.\n",
    "        :param n_features: SelectPercentile if float else SelectKBest\n",
    "        :param problem_type: classification or regression\n",
    "        :param scoring: scoring function, string\n",
    "        \"\"\"\n",
    "        if problem_type == \"classification\":\n",
    "            valid_scoring = {\n",
    "                \"f_classif\": f_classif,\n",
    "                \"chi2\": chi2,\n",
    "                \"mutual_info_classif\": mutual_info_classif\n",
    "            }\n",
    "\n",
    "        # for a given problem type, there are only\n",
    "        # a few valid scoring methods\n",
    "        # you can extend this with your own custom\n",
    "        # methods if you wish\n",
    "\n",
    "        else:\n",
    "            valid_scoring = {\n",
    "                \"f_regression\": f_regression,\n",
    "                \"mutual_info_regression\": mutual_info_regression\n",
    "             }\n",
    "\n",
    "        # raise exception if we do not have a valid scoring method\n",
    "        if scoring not in valid_scoring:\n",
    "            raise Exception(\"Invalid scoring function\")\n",
    "\n",
    "        # if n_features is int, we use selectkbest\n",
    "        # if n_features is float, we use selectpercentile\n",
    "        # please note that it is int in both cases in sklearn\n",
    "        \n",
    "        if isinstance(n_features, int):\n",
    "            self.selection = SelectKBest(\n",
    "                valid_scoring[scoring],\n",
    "                k=n_features\n",
    "            )\n",
    "\n",
    "        elif isinstance(n_features, float):\n",
    "            self.selection = SelectPercentile(\n",
    "                valid_scoring[scoring],\n",
    "                percentile=int(n_features * 100)\n",
    "            )\n",
    "        \n",
    "\n",
    "        else:\n",
    "            raise Exception(\"Invalid type of feature\")\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        # same fit function\n",
    "    def fit(self, X, y):\n",
    "        return self.selection.fit(X, y)\n",
    "\n",
    "        # same transform function\n",
    "    def transform(self, X):\n",
    "        return self.selection.transform(X)\n",
    "\n",
    "        # same fit_transform function\n",
    "    def fit_transform(self, X, y):\n",
    "        return self.selection.fit_transform(X, y)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "ufs = UnivariateFeatureSelction(\n",
    " n_features=0.1,\n",
    " problem_type=\"regression\",\n",
    " scoring=\"f_regression\"\n",
    ")\n",
    "ufs.fit(X, y)\n",
    "X_transformed = ufs.transform(X)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GreedyFeatureSelection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "\n",
    "class GreedyFeatureSelection:\n",
    "    \"\"\"\n",
    "    A simple and custom class for greedy feature selection.\n",
    "    You will need to modify it quite a bit to make it suitable\n",
    "    for your dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def evaluate_score(self, X, y):\n",
    "          \"\"\"\n",
    "        This function evaluates model on data and returns\n",
    "        Area Under ROC Curve (AUC)\n",
    "        NOTE: We fit the data and calculate AUC on same data.\n",
    "        WE ARE OVERFITTING HERE.\n",
    "        But this is also a way to achieve greedy selection.\n",
    "        k-fold will take k times longer.\n",
    "        If you want to implement it in really correct way,\n",
    "        calculate OOF AUC and return mean AUC over k folds.\n",
    "        This requires only a few lines of change and has been\n",
    "        shown a few times in this book.\n",
    "        :param X: training data\n",
    "        :param y: targets\n",
    "        :return: overfitted area under the roc curve\n",
    "        \"\"\"\n",
    "\n",
    "        # fit the logistic regression model,\n",
    "        # and calculate AUC on same data\n",
    "        # again: BEWARE\n",
    "        # you can choose any model that suits your data\n",
    "\n",
    "\n",
    "        model = linear_model.LogisticRegression()\n",
    "        model.fit(X, y)\n",
    "        predictions = model.predict_proba(X)[:, 1]\n",
    "        auc = metrics.roc_auc_score(y, predictions)\n",
    "        return auc\n",
    "\n",
    "\n",
    "    def _feature_selection(self, X, y):\n",
    "        \"\"\"\n",
    "        This function does the actual greedy selection\n",
    "        :param X: data, numpy array\n",
    "        :param y: targets, numpy array\n",
    "        :return: (best scores, best features)\n",
    "        \"\"\"\n",
    "        # initialize good features list\n",
    "        # and best scores to keep track of both\n",
    "        good_features = []\n",
    "        best_scores = []\n",
    "\n",
    "        # calculate the number of features\n",
    "        num_features = X.shape[1]\n",
    "\n",
    "        # infinite loop\n",
    "        while True:\n",
    "            # initialize best feature and score of this loop\n",
    "            this_feature = None\n",
    "            best_score = 0\n",
    "            # loop over all features\n",
    "            for feature in range(num_features):\n",
    "                # if feature is already in good features,\n",
    "                # skip this for loop\n",
    "                if feature in good_features:\n",
    "                    continue\n",
    "                # selected features are all good features till now\n",
    "                # and current feature\n",
    "                selected_features = good_features + [feature]\n",
    "                # remove all other features from data\n",
    "                xtrain = X[:, selected_features]\n",
    "                # calculate the score, in our case, AUC\n",
    "                score = self.evaluate_score(xtrain, y)\n",
    "                # if score is greater than the best score\n",
    "                # of this loop, change best score and best feature\n",
    "                if score > best_score:\n",
    "                    this_feature = feature\n",
    "                    best_score = score\n",
    "\n",
    "            # if we have selected a feature, add it\n",
    "            # to the good feature list and update best scores list\n",
    "            if this_feature != None:\n",
    "                good_features.append(this_feature)\n",
    "                best_scores.append(best_score)\n",
    "            # if we didnt improve during the last two rounds,\n",
    "            # exit the while loop\n",
    "            if len(best_scores) > 2:\n",
    "                if best_scores[-1] < best_scores[-2]:\n",
    "                    break\n",
    "        # return best scores and good features\n",
    "        # why do we remove the last data point?\n",
    "        return best_scores[:-1], good_features[:-1]\n",
    "\n",
    "\n",
    "    def __call__(self, X, y):\n",
    "        \"\"\"\n",
    "        Call function will call the class on a set of arguments\n",
    "        \"\"\"\n",
    "        # select features, return scores and selected indices\n",
    "        scores, features = self._feature_selection(X, y)\n",
    "        # transform data with selected features\n",
    "        return X[:, features], scores\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # generate binary classification data\n",
    "    X, y = make_classification(n_samples=1000, n_features=100)\n",
    "    # transform data by greedy feature selection\n",
    "    X_transformed, scores = GreedyFeatureSelection()(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "# fetch a regression dataset\n",
    "data = fetch_california_housing()\n",
    "X = data[\"data\"]\n",
    "col_names = data[\"feature_names\"]\n",
    "y = data[\"target\"]\n",
    "# initialize the model\n",
    "model = LinearRegression()\n",
    "# initialize RFE\n",
    "rfe = RFE(\n",
    " estimator=model,\n",
    " n_features_to_select=3\n",
    ")\n",
    "# fit RFE\n",
    "rfe.fit(X, y)\n",
    "# get the transformed data with\n",
    "# selected columns\n",
    "X_transformed = rfe.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Importance of Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#. All tree-based models provide feature importance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "# fetch a regression dataset\n",
    "# in diabetes data we predict diabetes progression\n",
    "# after one year based on some features\n",
    "data = load_diabetes()\n",
    "X = data[\"data\"]\n",
    "col_names = data[\"feature_names\"]\n",
    "y = data[\"target\"]\n",
    "# initialize the model\n",
    "model = RandomForestRegressor()\n",
    "# select from the model\n",
    "sfm = SelectFromModel(estimator=model)\n",
    "X_transformed = sfm.fit_transform(X, y)\n",
    "# see which features were selected\n",
    "support = sfm.get_support()\n",
    "# get feature names\n",
    "print([\n",
    " x for x, y in zip(col_names, support) if y == True\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
