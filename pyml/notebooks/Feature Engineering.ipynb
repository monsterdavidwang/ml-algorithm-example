{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering on Temploral Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_features(df):\n",
    "    # create a bunch of features using the date column\n",
    "    df.loc[:, 'year'] = df['date'].dt.year\n",
    "    df.loc[:, 'weekofyear'] = df['date'].dt.weekofyear\n",
    "    df.loc[:, 'month'] = df['date'].dt.month\n",
    "    df.loc[:, 'dayofweek'] = df['date'].dt.dayofweek\n",
    "    df.loc[:, 'weekend'] = (df['date'].dt.weekday >=5).astype(int)\n",
    "    # create an aggregate dictionary\n",
    "    aggs = {}\n",
    "    # for aggregation by month, we calculate the\n",
    "    # number of unique month values and also the mean\n",
    "    aggs['month'] = ['nunique', 'mean']\n",
    "    aggs['weekofyear'] = ['nunique', 'mean']\n",
    "    # we aggregate by num1 and calculate sum, max, min\n",
    "    # and mean values of this column\n",
    "    aggs['num1'] = ['sum','max','min','mean']\n",
    "    # for customer_id, we calculate the total count\n",
    "    aggs['customer_id'] = ['size']\n",
    "    # again for customer_id, we calculate the total unique\n",
    "    aggs['customer_id'] = ['nunique']\n",
    "    # we group by customer_id and calculate the aggregates\n",
    "    agg_df = df.groupby('customer_id').agg(aggs)\n",
    "    agg_df = agg_df.reset_index()\n",
    "    return agg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering on Numeric Data:\n",
    "\n",
    "1. Binarization\n",
    "2. Rounding\n",
    "3. Interactions(multiply , divided , add and substract features)(Make nonliner features (polynomial, sin , cos & tan))\n",
    "4. Binning\n",
    "5. Mathematical Transformations(Log transform,Box–Cox transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple way to generate many features is just to create a bunch of polynomial features. For\n",
    "example, a second-degree polynomial feature from two features “a” and “b” would\n",
    "include: “a”, “b”, “ab”, “a2” and “b2”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "# initialize polynomial features class object\n",
    "# for two-degree polynomial features\n",
    "pf = preprocessing.PolynomialFeatures(\n",
    "degree=2,\n",
    "interaction_only=False,\n",
    "include_bias=False\n",
    ")\n",
    "# fit to the features\n",
    "pf.fit(df)\n",
    "# create polynomial features\n",
    "poly_feats = pf.transform(df)\n",
    "# create a dataframe with all the features\n",
    "num_feats = poly_feats.shape[1]\n",
    "df_transformed = pd.DataFrame(\n",
    "poly_feats,\n",
    "columns=[f\"f_{i}\" for i in range(1, num_feats + 1)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another interesting feature converts the numbers to categories. It’s known as\n",
    "binning. Let’s look at figure 5, which shows a sample histogram of a random\n",
    "numerical feature. We use ten bins for this figure, and we see that we can divide the\n",
    "data into ten parts. This is accomplished using the pandas’ cut function.When you bin, you can use both the bin and the original feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create bins of the numerical columns\n",
    "# 10 bins\n",
    "df[\"f_bin_10\"] = pd.cut(df[\"f_1\"], bins=10, labels=False)\n",
    "# 100 bins\n",
    "df[\"f_bin_100\"] = pd.cut(df[\"f_1\"], bins=100, labels=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yet another interesting type of feature that you can create from numerical features\n",
    "is log transformation. f_3 is a special feature with a very high variance. Compared to other features that\n",
    "have a low variance (let’s assume that). Thus, we would want to reduce the variance\n",
    "of this column, and that can be done by taking a log transformation.Binning also enables you to treat\n",
    "numerical features as categorical.\n",
    "\n",
    "\n",
    "Sometimes, instead of log, you can also take exponential. A very interesting case is\n",
    "when you use a log-based evaluation metric, for example, RMSLE. In that case,\n",
    "you can train on log-transformed targets and convert back to original using\n",
    "exponential on the prediction. That would help optimize the model for the metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.f_3.apply(lambda x: np.log(1 + x)).var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Feature Engineering time series data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "feature_dict = {}\n",
    "# calculate mean\n",
    "feature_dict['mean'] = np.mean(x)\n",
    "# calculate max\n",
    "feature_dict['max'] = np.max(x)\n",
    "# calculate min\n",
    "feature_dict['min'] = np.min(x)\n",
    "# calculate standard deviation\n",
    "feature_dict['std'] = np.std(x)\n",
    "# calculate variance\n",
    "feature_dict['var'] = np.var(x)\n",
    "# peak-to-peak\n",
    "feature_dict['ptp'] = np.ptp(x)\n",
    "# percentile features\n",
    "feature_dict['percentile_10'] = np.percentile(x, 10)\n",
    "feature_dict['percentile_60'] = np.percentile(x, 60)\n",
    "feature_dict['percentile_90'] = np.percentile(x, 90)\n",
    "# quantile features\n",
    "feature_dict['quantile_5'] = np.percentile(x, 5)\n",
    "feature_dict['quantile_95'] = np.percentile(x, 95)\n",
    "feature_dict['quantile_99'] = np.percentile(x, 99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsfresh.feature_extraction import feature_calculators as fc\n",
    "# tsfresh based features\n",
    "feature_dict['abs_energy'] = fc.abs_energy(x)\n",
    "feature_dict['count_above_mean'] = fc.count_above_mean(x)\n",
    "feature_dict['count_below_mean'] = fc.count_below_mean(x)\n",
    "feature_dict['mean_abs_change'] = fc.mean_abs_change(x)\n",
    "feature_dict['mean_change'] = fc.mean_change(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering on Categorical Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Encoding and OneHot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "# create random 1-d array with 1001 different categories (int)\n",
    "example = np.random.randint(1000, size=1000000)\n",
    "# initialize OneHotEncoder from scikit-learn\n",
    "# keep sparse = False to get dense array\n",
    "ohe = preprocessing.OneHotEncoder(sparse=False)\n",
    "# fit and transform data with dense one hot encoder\n",
    "ohe_example = ohe.fit_transform(example.reshape(-1, 1))\n",
    "# print size in bytes for dense array\n",
    "print(f\"Size of dense array: {ohe_example.nbytes}\")\n",
    "# initialize OneHotEncoder from scikit-learn\n",
    "# keep sparse = True to get sparse array\n",
    "ohe = preprocessing.OneHotEncoder(sparse=True)\n",
    "# fit and transform data with sparse one-hot encoder\n",
    "ohe_example = ohe.fit_transform(example.reshape(-1, 1))\n",
    "# print size of this sparse matrix\n",
    "print(f\"Size of sparse array: {ohe_example.data.nbytes}\")\n",
    "full_size = (\n",
    "ohe_example.data.nbytes +\n",
    "ohe_example.indptr.nbytes + ohe_example.indices.nbytes\n",
    ")\n",
    "# print full size of this sparse matrix\n",
    "print(f\"Full size of sparse array: {full_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "# read training data\n",
    "train = pd.read_csv(\"../input/cat_train.csv\")\n",
    "#read test data\n",
    "test = pd.read_csv(\"../input/cat_test.csv\")\n",
    "# create a fake target column for test data\n",
    "# since this column doesn't exist\n",
    "test.loc[:, \"target\"] = -1\n",
    "# concatenate both training and test data\n",
    "data = pd.concat([train, test]).reset_index(drop=True)\n",
    "# make a list of features we are interested in\n",
    "# id and target is something we should not encode\n",
    "features = [x for x in train.columns if x not in [\"id\", \"target\"]]\n",
    "# loop over the features list\n",
    "for feat in features:\n",
    "# create a new instance of LabelEncoder for each feature\n",
    "lbl_enc = preprocessing.LabelEncoder()\n",
    "# note the trick here\n",
    "# since its categorical data, we fillna with a string\n",
    "# and we convert all the data to string type\n",
    "# so, no matter its int or float, its converted to string\n",
    "# int/float but categorical!!!\n",
    "temp_col = data[feat].fillna(\"NONE\").astype(str).values\n",
    "# we can use fit_transform here as we do not\n",
    "# have any extra test data that we need to\n",
    "# transform on separately\n",
    "data.loc[:, feat] = lbl_enc.fit_transform(temp_col)\n",
    "# split the training and test data again\n",
    "train = data[data.target != -1].reset_index(drop=True)\n",
    "test = data[data.target == -1].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ohe_logres.py\n",
    "import pandas as pd\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "def run(fold):\n",
    "    # load the full training data with folds\n",
    "    df = pd.read_csv(\"../input/cat_train_folds.csv\")\n",
    "    # all columns are features except id, target and kfold columns\n",
    "    features = [\n",
    "        f for f in df.columns if f not in (\"id\", \"target\", \"kfold\")\n",
    "    ]\n",
    "    # fill all NaN values with NONE \n",
    "    # note that I am converting all columns to \"strings\"\n",
    "    # it doesn’t matter because all are categories\n",
    "    for col in features:\n",
    "        df.loc[:, col] = df[col].astype(str).fillna(\"NONE\")\n",
    "    # get training data using folds\n",
    "    df_train = df[df.kfold != fold].reset_index(drop=True)\n",
    "    # get validation data using folds\n",
    "    df_valid = df[df.kfold == fold].reset_index(drop=True)\n",
    "    # initialize OneHotEncoder from scikit-learn\n",
    "    ohe = preprocessing.OneHotEncoder()\n",
    "    # fit ohe on training + validation features\n",
    "    full_data = pd.concat([df_train[features], df_valid[features]],axis=0)\n",
    "    \n",
    "    ohe.fit(full_data[features])\n",
    "    # transform training data\n",
    "    x_train = ohe.transform(df_train[features])\n",
    "    # transform validation data\n",
    "    x_valid = ohe.transform(df_valid[features])\n",
    "    # initialize Logistic Regression model\n",
    "    model = linear_model.LogisticRegression()\n",
    "    # fit model on training data (ohe)\n",
    "    model.fit(x_train, df_train.target.values)\n",
    "    # predict on validation data\n",
    "    # we need the probability values as we are calculating AUC\n",
    "    # we will use the probability of 1s\n",
    "    valid_preds = model.predict_proba(x_valid)[:, 1]\n",
    "    # get roc auc score\n",
    "    auc = metrics.roc_auc_score(df_valid.target.values, valid_preds)\n",
    "    # print auc\n",
    "    print(auc)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # run function for fold = 0\n",
    "    # we can just replace this number and\n",
    "    # run this for any fold\n",
    "    run(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lbl_rf.py\n",
    "import pandas as pd\n",
    "from sklearn import ensemble\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "\n",
    "def run(fold):\n",
    "    # load the full training data with folds\n",
    "    df = pd.read_csv(\"../input/cat_train_folds.csv\")\n",
    "    # all columns are features except id, target and kfold columns\n",
    "    features = [\n",
    "    f for f in df.columns if f not in (\"id\", \"target\", \"kfold\")\n",
    "    ]\n",
    "    # fill all NaN values with NONE\n",
    "    # note that I am converting all columns to \"strings\"\n",
    "    # it doesnt matter because all are categories\n",
    "    for col in features:\n",
    "        df.loc[:, col] = df[col].astype(str).fillna(\"NONE\")\n",
    "    # now its time to label encode the features\n",
    "    for col in features:\n",
    "        # initialize LabelEncoder for each feature column\n",
    "        lbl = preprocessing.LabelEncoder()\n",
    "        # fit label encoder on all data\n",
    "        lbl.fit(df[col])\n",
    "        # transform all the data\n",
    "        df.loc[:, col] = lbl.transform(df[col])\n",
    "    # get training data using folds\n",
    "    df_train = df[df.kfold != fold].reset_index(drop=True)\n",
    "    # get validation data using folds\n",
    "    df_valid = df[df.kfold == fold].reset_index(drop=True)\n",
    "    # get training data\n",
    "    x_train = df_train[features].values\n",
    "    # get validation data\n",
    "    x_valid = df_valid[features].values\n",
    "    # initialize random forest model\n",
    "    model = ensemble.RandomForestClassifier(n_jobs=-1)\n",
    "    # fit model on training data (ohe)\n",
    "    model.fit(x_train, df_train.target.values)\n",
    "    # predict on validation data\n",
    "    # we need the probability values as we are calculating AUC\n",
    "    # we will use the probability of 1s\n",
    "    valid_preds = model.predict_proba(x_valid)[:, 1]\n",
    "    # get roc auc score\n",
    "    auc = metrics.roc_auc_score(df_valid.target.values, valid_preds)\n",
    "    # print auc\n",
    "    print(f\"Fold = {fold}, AUC = {auc}\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    for fold_ in range(5):\n",
    "        run(fold_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ohe_svd_rf.py\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "from sklearn import decomposition\n",
    "from sklearn import ensemble\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "def run(fold):\n",
    "    # load the full training data with folds\n",
    "    df = pd.read_csv(\"../input/cat_train_folds.csv\")\n",
    "    # all columns are features except id, target and kfold columns\n",
    "    features = [\n",
    "        f for f in df.columns if f not in (\"id\", \"target\", \"kfold\")\n",
    "    ]\n",
    "    # fill all NaN values with NONE\n",
    "    # note that I am converting all columns to \"strings\"\n",
    "    # it doesnt matter because all are categories\n",
    "    for col in features:\n",
    "        df.loc[:, col] = df[col].astype(str).fillna(\"NONE\")\n",
    "    # get training data using folds\n",
    "    df_train = df[df.kfold != fold].reset_index(drop=True)\n",
    "    # get validation data using folds\n",
    "    df_valid = df[df.kfold == fold].reset_index(drop=True)\n",
    "    # initialize OneHotEncoder from scikit-learn\n",
    "    ohe = preprocessing.OneHotEncoder()\n",
    "    # fit ohe on training + validation features\n",
    "    full_data = pd.concat(\n",
    "        [df_train[features], df_valid[features]],\n",
    "        axis=0\n",
    "    )\n",
    "    ohe.fit(full_data[features])\n",
    "    # transform training data\n",
    "    x_train = ohe.transform(df_train[features])\n",
    "    # transform validation data\n",
    "    x_valid = ohe.transform(df_valid[features])\n",
    "    # initialize Truncated SVD\n",
    "    # we are reducing the data to 120 components\n",
    "    svd = decomposition.TruncatedSVD(n_components=120)\n",
    "    # fit svd on full sparse training data\n",
    "    full_sparse = sparse.vstack((x_train, x_valid))\n",
    "    svd.fit(full_sparse)\n",
    "    # transform sparse training data\n",
    "    x_train = svd.transform(x_train)\n",
    "    # transform sparse validation data\n",
    "    x_valid = svd.transform(x_valid)\n",
    "    # initialize random forest model\n",
    "    model = ensemble.RandomForestClassifier(n_jobs=-1)\n",
    "    # fit model on training data (ohe)\n",
    "    model.fit(x_train, df_train.target.values)\n",
    "    # predict on validation data\n",
    "    # we need the probability values as we are calculating AUC\n",
    "    # we will use the probability of 1s\n",
    "    valid_preds = model.predict_proba(x_valid)[:, 1]\n",
    "    # get roc auc score\n",
    "    auc = metrics.roc_auc_score(df_valid.target.values, valid_preds)\n",
    "    # print auc\n",
    "    print(f\"Fold = {fold}, AUC = {auc}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for fold_ in range(5):\n",
    "        run(fold_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lbl_xgb.py\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "def run(fold):\n",
    "    # load the full training data with folds\n",
    "    df = pd.read_csv(\"../input/cat_train_folds.csv\")\n",
    "    # all columns are features except id, target and kfold columns\n",
    "    features = [\n",
    "        f for f in df.columns if f not in (\"id\", \"target\", \"kfold\")\n",
    "    ]\n",
    "    # fill all NaN values with NONE\n",
    "    # note that I am converting all columns to \"strings\"\n",
    "    # it doesnt matter because all are categories\n",
    "    for col in features:\n",
    "        df.loc[:, col] = df[col].astype(str).fillna(\"NONE\")\n",
    "    # now it’s time to label encode the features\n",
    "    for col in features:\n",
    "        # initialize LabelEncoder for each feature column\n",
    "        lbl = preprocessing.LabelEncoder()\n",
    "        # fit label encoder on all data\n",
    "        lbl.fit(df[col])\n",
    "        # transform all the data\n",
    "        df.loc[:, col] = lbl.transform(df[col])\n",
    "    # get training data using folds\n",
    "    df_train = df[df.kfold != fold].reset_index(drop=True)\n",
    "    # get validation data using folds\n",
    "    df_valid = df[df.kfold == fold].reset_index(drop=True)\n",
    "    # get training data\n",
    "    x_train = df_train[features].values\n",
    "    # get validation data\n",
    "    x_valid = df_valid[features].values\n",
    "    # initialize xgboost model\n",
    "    model = xgb.XGBClassifier(\n",
    "    n_jobs=-1,\n",
    "    max_depth=7,\n",
    "    n_estimators=200\n",
    "    )\n",
    "    # fit model on training data (ohe)\n",
    "    model.fit(x_train, df_train.target.values)\n",
    "    # predict on validation data\n",
    "    # we need the probability values as we are calculating AUC\n",
    "    # we will use the probability of 1s\n",
    "    valid_preds = model.predict_proba(x_valid)[:, 1]\n",
    "    # get roc auc score\n",
    "    auc = metrics.roc_auc_score(df_valid.target.values, valid_preds)\n",
    "    # print auc\n",
    "    print(f\"Fold = {fold}, AUC = {auc}\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    for fold_ in range(5):\n",
    "        run(fold_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ohe_logres.py\n",
    "import pandas as pd\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "def run(fold):\n",
    "    # load the full training data with folds\n",
    "    df = pd.read_csv(\"../input/cat_train_folds.csv\")\n",
    "    # all columns are features except id, target and kfold columns\n",
    "    features = [\n",
    "        f for f in df.columns if f not in (\"id\", \"target\", \"kfold\")\n",
    "    ]\n",
    "    # fill all NaN values with NONE \n",
    "    # note that I am converting all columns to \"strings\"\n",
    "    # it doesn’t matter because all are categories\n",
    "    for col in features:\n",
    "        df.loc[:, col] = df[col].astype(str).fillna(\"NONE\")\n",
    "    # get training data using folds\n",
    "    df_train = df[df.kfold != fold].reset_index(drop=True)\n",
    "    # get validation data using folds\n",
    "    df_valid = df[df.kfold == fold].reset_index(drop=True)\n",
    "    # initialize OneHotEncoder from scikit-learn\n",
    "    ohe = preprocessing.OneHotEncoder()\n",
    "    # fit ohe on training + validation features\n",
    "    full_data = pd.concat(\n",
    "        [df_train[features], df_valid[features]],\n",
    "        axis=0\n",
    "    )\n",
    "    ohe.fit(full_data[features])\n",
    "    # transform training data\n",
    "    x_train = ohe.transform(df_train[features])\n",
    "    # transform validation data\n",
    "    x_valid = ohe.transform(df_valid[features])\n",
    "    # initialize Logistic Regression model\n",
    "    model = linear_model.LogisticRegression()\n",
    "    # fit model on training data (ohe)\n",
    "    model.fit(x_train, df_train.target.values)\n",
    "    # predict on validation data\n",
    "    # we need the probability values as we are calculating AUC\n",
    "    # we will use the probability of 1s\n",
    "    valid_preds = model.predict_proba(x_valid)[:, 1]\n",
    "    # get roc auc score\n",
    "    auc = metrics.roc_auc_score(df_valid.target.values, valid_preds)\n",
    "    # print auc\n",
    "    print(auc)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # run function for fold = 0\n",
    "    # we can just replace this number and\n",
    "    # run this for any fold\n",
    "    run(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lbl_xgb_num.py\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "\n",
    "def run(fold):\n",
    "    # load the full training data with folds\n",
    "    df = pd.read_csv(\"../input/adult_folds.csv\")\n",
    "    # list of numerical columns\n",
    "    num_cols = [\n",
    "    \"fnlwgt\",\n",
    "    \"age\",\n",
    "    \"capital.gain\",\n",
    "    \"capital.loss\",\n",
    "    \"hours.per.week\"\n",
    "    ]\n",
    "    # map targets to 0s and 1s\n",
    "    target_mapping = {\n",
    "    \"<=50K\": 0,\n",
    "    \">50K\": 1\n",
    "    }\n",
    "    df.loc[:, \"income\"] = df.income.map(target_mapping)\n",
    "    # all columns are features except kfold & income columns\n",
    "    features = [\n",
    "    f for f in df.columns if f not in (\"kfold\", \"income\")\n",
    "    ]\n",
    "    # fill all NaN values with NONE\n",
    "    # note that I am converting all columns to \"strings\"\n",
    "    # it doesnt matter because all are categories\n",
    "    for col in features:\n",
    "        # do not encode the numerical columns\n",
    "        if col not in num_cols:\n",
    "            df.loc[:, col] = df[col].astype(str).fillna(\"NONE\")\n",
    "    # now its time to label encode the features\n",
    "    for col in features:\n",
    "        if col not in num_cols:\n",
    "            # initialize LabelEncoder for each feature column\n",
    "            lbl = preprocessing.LabelEncoder()\n",
    "            # fit label encoder on all data\n",
    "            lbl.fit(df[col])\n",
    "            # transform all the data\n",
    "            df.loc[:, col] = lbl.transform(df[col])\n",
    "    # get training data using folds\n",
    "    df_train = df[df.kfold != fold].reset_index(drop=True)\n",
    "    # get validation data using folds\n",
    "    df_valid = df[df.kfold == fold].reset_index(drop=True)\n",
    "    # get training data\n",
    "    x_train = df_train[features].values\n",
    "    # get validation data\n",
    "    x_valid = df_valid[features].values\n",
    "    # initialize xgboost model\n",
    "    model = xgb.XGBClassifier(\n",
    "    n_jobs=-1\n",
    "    )\n",
    "    # fit model on training data (ohe)\n",
    "    model.fit(x_train, df_train.income.values)\n",
    "    # predict on validation data\n",
    "    # we need the probability values as we are calculating AUC\n",
    "    # we will use the probability of 1s\n",
    "    valid_preds = model.predict_proba(x_valid)[:, 1]\n",
    "    # get roc auc score\n",
    "    auc = metrics.roc_auc_score(df_valid.income.values, valid_preds)\n",
    "    # print auc\n",
    "    print(f\"Fold = {fold}, AUC = {auc}\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    for fold_ in range(5):\n",
    "        run(fold_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lbl_xgb_num_feat.py\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "def feature_engineering(df, cat_cols):\n",
    "    \"\"\"\n",
    "    This function is used for feature engineering\n",
    "    :param df: the pandas dataframe with train/test data\n",
    "    :param cat_cols: list of categorical columns\n",
    "    :return: dataframe with new features\n",
    "    \"\"\"\n",
    "    # this will create all 2-combinations of values\n",
    "    # in this list\n",
    "    # for example:\n",
    "    # list(itertools.combinations([1,2,3], 2)) will return\n",
    "    # [(1, 2), (1, 3), (2, 3)]\n",
    "    combi = list(itertools.combinations(cat_cols, 2))\n",
    "    for c1, c2 in combi:\n",
    "    df.loc[:,c1 + \"_\" + c2] = df[c1].astype(str) + \"_\" + df[c2].astype(str)\n",
    "    return df\n",
    "\n",
    "def run(fold):\n",
    "    # load the full training data with folds\n",
    "    df = pd.read_csv(\"../input/adult_folds.csv\")\n",
    "    # list of numerical columns\n",
    "    num_cols = [\"fnlwgt\",\"age\",\"capital.gain\",\"capital.loss\",\"hours.per.week\"]\n",
    "    # map targets to 0s and 1s\n",
    "    target_mapping = {\n",
    "    \"<=50K\": 0,\n",
    "    \">50K\": 1\n",
    "    }\n",
    "    df.loc[:, \"income\"] = df.income.map(target_mapping)\n",
    "    # list of categorical columns for feature engineering\n",
    "    cat_cols = [c for c in df.columns if c not in num_colsand c not in (\"kfold\", \"income\")]\n",
    "    # add new features\n",
    "    df = feature_engineering(df, cat_cols)\n",
    "    # all columns are features except kfold & income columns\n",
    "    features = [f for f in df.columns if f not in (\"kfold\", \"income\")]\n",
    "    # fill all NaN values with NONE\n",
    "    # note that I am converting all columns to \"strings\"\n",
    "    # it doesnt matter because all are categories\n",
    "    for col in features:\n",
    "    # do not encode the numerical columns\n",
    "        if col not in num_cols:\n",
    "            df.loc[:, col] = df[col].astype(str).fillna(\"NONE\")\n",
    "    # now its time to label encode the features\n",
    "    for col in features:\n",
    "        if col not in num_cols:\n",
    "            # initialize LabelEncoder for each feature column\n",
    "            lbl = preprocessing.LabelEncoder()\n",
    "            # fit label encoder on all data\n",
    "            lbl.fit(df[col])\n",
    "            # transform all the data\n",
    "            df.loc[:, col] = lbl.transform(df[col])\n",
    "    # get training data using folds\n",
    "    df_train = df[df.kfold != fold].reset_index(drop=True)\n",
    "    # get validation data using folds\n",
    "    df_valid = df[df.kfold == fold].reset_index(drop=True)\n",
    "    # get training data\n",
    "    x_train = df_train[features].values\n",
    "    # get validation data\n",
    "    x_valid = df_valid[features].values\n",
    "    # initialize xgboost model\n",
    "    model = xgb.XGBClassifier(\n",
    "    n_jobs=-1\n",
    "    )\n",
    "    # fit model on training data (ohe)\n",
    "    model.fit(x_train, df_train.income.values)\n",
    "    # predict on validation data\n",
    "    # we need the probability values as we are calculating AUC\n",
    "    # we will use the probability of 1s\n",
    "    valid_preds = model.predict_proba(x_valid)[:, 1]\n",
    "    # get roc auc score\n",
    "    auc = metrics.roc_auc_score(df_valid.income.values, valid_preds)\n",
    "    # print auc\n",
    "    print(f\"Fold = {fold}, AUC = {auc}\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    for fold_ in range(5):\n",
    "        run(fold_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "One more way of feature engineering from categorical features is to use target\n",
    "encoding. However, you have to be very careful here as this might overfit your\n",
    "model. Target encoding is a technique in which you map each category in a given\n",
    "feature to its mean target value, but this must always be done in a cross-validated\n",
    "manner. It means that the first thing you do is create the folds, and then use those\n",
    "folds to create target encoding features for different columns of the data in the same\n",
    "way you fit and predict the model on folds. So, if you have created 5 folds, you have\n",
    "to create target encoding 5 times such that in the end, you have encoding for\n",
    "variables in each fold which are not derived from the same fold. And then when\n",
    "you fit your model, you must use the same folds again. Target encoding for unseen\n",
    "test data can be derived from the full training data or can be an average of all the 5\n",
    "folds. \n",
    "\n",
    "When we use target\n",
    "encoding, it’s better to use some kind of smoothing or adding noise in the encoded\n",
    "values. Scikit-learn has contrib repository which has target encoding with\n",
    "smoothing, or you can create your own smoothing. Smoothing introduces some\n",
    "kind of regularization that helps with not overfitting the model. It’s not very\n",
    "difficult."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "def mean_target_encoding(data):\n",
    "    # make a copy of dataframe\n",
    "    df = copy.deepcopy(data)\n",
    "    # list of numerical columns\n",
    "    num_cols = [\n",
    "        \"fnlwgt\",\n",
    "        \"age\",\n",
    "        \"capital.gain\",\n",
    "        \"capital.loss\",\n",
    "        \"hours.per.week\"\n",
    "    ]\n",
    "    # map targets to 0s and 1s\n",
    "    target_mapping = {\n",
    "        \"<=50K\": 0,\n",
    "        \">50K\": 1\n",
    "    }\n",
    "    df.loc[:, \"income\"] = df.income.map(target_mapping)\n",
    "    # all columns are features except income and kfold columns\n",
    "    features = [\n",
    "        f for f in df.columns if f not in (\"kfold\", \"income\") and f not in num_cols\n",
    "    ]\n",
    "    # all columns are features except kfold & income columns\n",
    "    features = [\n",
    "        f for f in df.columns if f not in (\"kfold\", \"income\")\n",
    "    ]\n",
    "    # fill all NaN values with NONE\n",
    "    # note that I am converting all columns to \"strings\"\n",
    "    # it doesnt matter because all are categories\n",
    "    for col in features:\n",
    "        # do not encode the numerical columns\n",
    "        if col not in num_cols:\n",
    "            df.loc[:, col] = df[col].astype(str).fillna(\"NONE\")\n",
    "        # now its time to label encode the features\n",
    "    for col in features:\n",
    "        if col not in num_cols:\n",
    "            # initialize LabelEncoder for each feature column\n",
    "            lbl = preprocessing.LabelEncoder()\n",
    "            # fit label encoder on all data\n",
    "            lbl.fit(df[col])\n",
    "            # transform all the data\n",
    "            df.loc[:, col] = lbl.transform(df[col])\n",
    "    # a list to store 5 validation dataframes\n",
    "    encoded_dfs = []\n",
    "    # go over all folds\n",
    "    for fold in range(5):\n",
    "        # fetch training and validation data\n",
    "        df_train = df[df.kfold != fold].reset_index(drop=True)\n",
    "        df_valid = df[df.kfold == fold].reset_index(drop=True)\n",
    "    # for all feature columns, i.e. categorical columns\n",
    "    for column in features:\n",
    "        # create dict of category:mean target\n",
    "        mapping_dict = dict(\n",
    "            df_train.groupby(column)[\"income\"].mean()\n",
    "        )\n",
    "    # column_enc is the new column we have with mean encoding\n",
    "    df_valid.loc[\n",
    "        :, column + \"_enc\"\n",
    "    ] = df_valid[column].map(mapping_dict)\n",
    "    # append to our list of encoded validation dataframes\n",
    "    encoded_dfs.append(df_valid)\n",
    "    # create full data frame again and return\n",
    "    encoded_df = pd.concat(encoded_dfs, axis=0)\n",
    "    return encoded_df\n",
    "\n",
    "\n",
    "def run(df, fold):\n",
    "\n",
    "\n",
    "    # note that folds are same as before\n",
    "    # get training data using folds\n",
    "df_train = df[df.kfold != fold].reset_index(drop=True)\n",
    "# get validation data using folds\n",
    "df_valid = df[df.kfold == fold].reset_index(drop=True)\n",
    "# all columns are features except income and kfold columns\n",
    "features = [\n",
    "    f for f in df.columns if f not in (\"kfold\", \"income\")\n",
    "]\n",
    "# scale training data\n",
    "x_train = df_train[features].values\n",
    "# scale validation data\n",
    "x_valid = df_valid[features].values\n",
    "# initialize xgboost model\n",
    "model = xgb.XGBClassifier(\n",
    "    n_jobs=-1,\n",
    "    max_depth=7\n",
    ")\n",
    "# fit model on training data (ohe)\n",
    "model.fit(x_train, df_train.income.values)\n",
    "# predict on validation data\n",
    "# we need the probability values as we are calculating AUC\n",
    "# we will use the probability of 1s\n",
    "valid_preds = model.predict_proba(x_valid)[:, 1]\n",
    "# get roc auc score\n",
    "auc = metrics.roc_auc_score(df_valid.income.values, valid_preds)\n",
    "# print auc\n",
    "print(f\"Fold = {fold}, AUC = {auc}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # read data\n",
    "    df = pd.read_csv(\"../input/adult_folds.csv\")\n",
    "    # create mean target encoded categories and\n",
    "    # munge data\n",
    "    df = mean_target_encoding(df)\n",
    "    # run training and validation for 5 folds\n",
    "    for fold_ in range(5):\n",
    "        run(df, fold_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entity Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The idea is super simple. You have an embedding layer for each categorical feature.\n",
    "So, every category in a column can now be mapped to an embedding (like mapping\n",
    "words to embeddings in natural language processing). You then reshape these\n",
    "embeddings to their dimension to make them flat and then concatenate all the\n",
    "flattened inputs embeddings. Then add a bunch of dense layers, an output layer and\n",
    "you are done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics, preprocessing\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import utils\n",
    "\n",
    "\n",
    "def create_model(data, catcols):\n",
    "    \"\"\"\n",
    "    This function returns a compiled tf.keras model\n",
    "    for entity embeddings\n",
    "    :param data: this is a pandas dataframe\n",
    "    :param catcols: list of categorical column names\n",
    "    :return: compiled tf.keras model\n",
    "    \"\"\"\n",
    "    # init list of inputs for embeddings\n",
    "    inputs = []\n",
    "    # init list of outputs for embeddings\n",
    "    outputs = []\n",
    "    # loop over all categorical columns\n",
    "    for c in catcols:\n",
    "        # find the number of unique values in the column\n",
    "        num_unique_values = int(data[c].nunique())\n",
    "        # simple dimension of embedding calculator\n",
    "        # min size is half of the number of unique values\n",
    "        # max size is 50. max size depends on the number of unique\n",
    "        # categories too. 50 is quite sufficient most of the times\n",
    "        # but if you have millions of unique values, you might need\n",
    "        # a larger dimension\n",
    "        embed_dim = int(min(np.ceil((num_unique_values)/2), 50))\n",
    "        # simple keras input layer with size 1\n",
    "        inp = layers.Input(shape=(1,))\n",
    "        # add embedding layer to raw input\n",
    "        # embedding size is always 1 more than unique values in input\n",
    "        out = layers.Embedding(\n",
    "        num_unique_values + 1, embed_dim, name=c\n",
    "        )(inp)\n",
    "        # 1-d spatial dropout is the standard for emebedding layers\n",
    "        # you can use it in NLP tasks too\n",
    "        out = layers.SpatialDropout1D(0.3)(out)\n",
    "        # reshape the input to the dimension of embedding\n",
    "        # this becomes our output layer for current feature\n",
    "        out = layers.Reshape(target_shape=(embed_dim, ))(out)\n",
    "        # add input to input list\n",
    "        inputs.append(inp)\n",
    "        # add output to output list\n",
    "        outputs.append(out)\n",
    "        # concatenate all output layers\n",
    "    x = layers.Concatenate()(outputs)\n",
    "    # add a batchnorm layer.\n",
    "    # from here, everything is up to you\n",
    "    # you can try different architectures\n",
    "    # this is the architecture I like to use\n",
    "    # if you have numerical features, you should add\n",
    "    # them here or in concatenate layer\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    # a bunch of dense layers with dropout.\n",
    "    # start with 1 or two layers only\n",
    "    x = layers.Dense(300, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dense(300, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    # using softmax and treating it as a two class problem\n",
    "    # you can also use sigmoid, then you need to use only one\n",
    "    # output class\n",
    "    y = layers.Dense(2, activation=\"softmax\")(x)\n",
    "    # create final model\n",
    "    model = Model(inputs=inputs, outputs=y)\n",
    "    # compile the model\n",
    "    # we use adam and binary cross entropy.\n",
    "    # feel free to use something else and see how model behaves\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def run(fold):\n",
    "    # load the full training data with folds\n",
    "    df = pd.read_csv(\"../input/cat_train_folds.csv\")\n",
    "    # all columns are features except id, target and kfold columns\n",
    "    features = [\n",
    "    f for f in df.columns if f not in (\"id\", \"target\", \"kfold\")\n",
    "    ]\n",
    "    # fill all NaN values with NONE\n",
    "    # note that I am converting all columns to \"strings\"\n",
    "    # it doesnt matter because all are categories\n",
    "    for col in features:\n",
    "        df.loc[:, col] = df[col].astype(str).fillna(\"NONE\")\n",
    "    # encode all features with label encoder individually\n",
    "    # in a live setting you need to save all label encoders\n",
    "    for feat in features:\n",
    "        lbl_enc = preprocessing.LabelEncoder()\n",
    "        df.loc[:, feat] = lbl_enc.fit_transform(df[feat].values)\n",
    "    # get training data using folds\n",
    "    df_train = df[df.kfold != fold].reset_index(drop=True)\n",
    "    # get validation data using folds\n",
    "    df_valid = df[df.kfold == fold].reset_index(drop=True)\n",
    "    # create tf.keras model\n",
    "    model = create_model(df, features)\n",
    "    # our features are lists of lists\n",
    "    xtrain = [\n",
    "    df_train[features].values[:, k] for k in range(len(features))\n",
    "    ]\n",
    "    xvalid = [\n",
    "    df_valid[features].values[:, k] for k in range(len(features))\n",
    "    ]\n",
    "    # fetch target columns\n",
    "    ytrain = df_train.target.values\n",
    "    yvalid = df_valid.target.values\n",
    "    # convert target columns to categories\n",
    "    # this is just binarization\n",
    "    ytrain_cat = utils.to_categorical(ytrain)\n",
    "    yvalid_cat = utils.to_categorical(yvalid)\n",
    "    # fit the model\n",
    "    model.fit(xtrain,\n",
    "    ytrain_cat,\n",
    "    validation_data=(xvalid, yvalid_cat),\n",
    "    verbose=1,\n",
    "    batch_size=1024,\n",
    "    epochs=3\n",
    "    )\n",
    "    # generate validation predictions\n",
    "    valid_preds = model.predict(xvalid)[:, 1]\n",
    "    # print roc auc score\n",
    "    print(metrics.roc_auc_score(yvalid, valid_preds))\n",
    "    # clear session to free up some GPU memory\n",
    "    K.clear_session()\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    run(0)\n",
    "    run(1)\n",
    "    run(2)\n",
    "    run(3)\n",
    "    run(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import MaxAbsScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
